{
  "timestamp": "2025-11-01T16:55:39.558489",
  "total_workers": 6,
  "completed": 4,
  "failed": 2,
  "success_rate": 0.6666666666666666,
  "duration_seconds": 0.008471488952636719,
  "workers": [
    {
      "id": "W1_METRICS",
      "name": "Debate Metrics & Analytics Collector",
      "code": "\nimport json\nfrom datetime import datetime\n\nclass DebateMetricsCollector:\n    \"\"\"Collects and analyzes debate metrics\"\"\"\n    \n    def __init__(self):\n        self.metrics = {\n            \"providers\": {},\n            \"arguments\": {},\n            \"performance\": {}\n        }\n    \n    def collect_provider_metrics(self, debate_results):\n        \"\"\"Collect per-provider metrics\"\"\"\n        for round_data in debate_results.get(\"rounds\", []):\n            for arg in round_data.get(\"arguments\", []):\n                provider = arg.get(\"provider\")\n                if provider not in self.metrics[\"providers\"]:\n                    self.metrics[\"providers\"][provider] = {\n                        \"total_arguments\": 0,\n                        \"total_latency_ms\": 0,\n                        \"total_tokens\": 0,\n                        \"avg_latency_ms\": 0,\n                        \"avg_tokens\": 0\n                    }\n                \n                p_metrics = self.metrics[\"providers\"][provider]\n                p_metrics[\"total_arguments\"] += 1\n                p_metrics[\"total_latency_ms\"] += arg.get(\"latency_ms\", 0)\n                p_metrics[\"total_tokens\"] += arg.get(\"token_count\", 0)\n                p_metrics[\"avg_latency_ms\"] = p_metrics[\"total_latency_ms\"] / p_metrics[\"total_arguments\"]\n                p_metrics[\"avg_tokens\"] = p_metrics[\"total_tokens\"] / p_metrics[\"total_arguments\"]\n        \n        return self.metrics\n    \n    def save_metrics(self, filename=\"debate_metrics.json\"):\n        \"\"\"Save metrics to file\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.metrics, f, indent=2)\n        return filename\n\n# Create metrics collector template\ncollector = DebateMetricsCollector()\nprint(\"[W1] Debate Metrics Collector created\")\nprint(f\"  - Provider metrics tracking: READY\")\nprint(f\"  - Performance analytics: READY\")\nprint(f\"  - Export functionality: READY\")\n",
      "dependencies": [],
      "status": "completed",
      "duration": 0.0013723373413085938
    },
    {
      "id": "W2_QUALITY",
      "name": "Argument Quality & Coherence Analyzer",
      "code": "\nimport re\nfrom typing import List, Dict\n\nclass ArgumentQualityAnalyzer:\n    \"\"\"Analyzes quality and coherence of debate arguments\"\"\"\n    \n    def __init__(self):\n        self.quality_metrics = {}\n    \n    def analyze_argument_quality(self, argument_text: str) -> Dict:\n        \"\"\"\n        Analyze single argument quality\n        \n        Metrics:\n        - Length (words)\n        - Sentence count\n        - Average sentence length\n        - Question count\n        - Citation indicators\n        - Logical connectors\n        \"\"\"\n        metrics = {}\n        \n        # Basic counts\n        words = argument_text.split()\n        metrics[\"word_count\"] = len(words)\n        \n        sentences = re.split(r'[.!?]+', argument_text)\n        sentences = [s.strip() for s in sentences if s.strip()]\n        metrics[\"sentence_count\"] = len(sentences)\n        \n        metrics[\"avg_sentence_length\"] = (\n            metrics[\"word_count\"] / metrics[\"sentence_count\"] \n            if metrics[\"sentence_count\"] > 0 else 0\n        )\n        \n        # Questions\n        metrics[\"question_count\"] = argument_text.count('?')\n        \n        # Citation indicators\n        citation_patterns = [\n            r'according to', r'research shows', r'studies indicate',\n            r'evidence suggests', r'data shows', r'\\d{4}',  # Years\n        ]\n        metrics[\"citation_indicators\"] = sum(\n            len(re.findall(pattern, argument_text, re.I))\n            for pattern in citation_patterns\n        )\n        \n        # Logical connectors\n        connectors = [\n            'however', 'therefore', 'furthermore', 'moreover',\n            'consequently', 'thus', 'hence', 'because', 'since'\n        ]\n        metrics[\"logical_connectors\"] = sum(\n            argument_text.lower().count(conn) for conn in connectors\n        )\n        \n        # Quality score (0-100)\n        quality_score = 0\n        quality_score += min(25, metrics[\"word_count\"] / 10)  # Length (max 25)\n        quality_score += min(25, metrics[\"citation_indicators\"] * 5)  # Citations (max 25)\n        quality_score += min(25, metrics[\"logical_connectors\"] * 3)  # Logic (max 25)\n        quality_score += min(25, (100 - abs(metrics[\"avg_sentence_length\"] - 20)))  # Readability (max 25)\n        \n        metrics[\"quality_score\"] = round(quality_score, 2)\n        \n        return metrics\n    \n    def analyze_debate_quality(self, debate_results: Dict) -> Dict:\n        \"\"\"Analyze quality across entire debate\"\"\"\n        all_metrics = []\n        \n        for round_data in debate_results.get(\"rounds\", []):\n            for arg in round_data.get(\"arguments\", []):\n                arg_metrics = self.analyze_argument_quality(arg.get(\"argument\", \"\"))\n                arg_metrics[\"provider\"] = arg.get(\"provider\")\n                arg_metrics[\"position\"] = arg.get(\"position\")\n                arg_metrics[\"round\"] = arg.get(\"round_number\")\n                all_metrics.append(arg_metrics)\n        \n        # Aggregate by provider\n        provider_quality = {}\n        for m in all_metrics:\n            provider = m[\"provider\"]\n            if provider not in provider_quality:\n                provider_quality[provider] = {\n                    \"avg_quality_score\": 0,\n                    \"avg_word_count\": 0,\n                    \"total_citations\": 0,\n                    \"total_connectors\": 0,\n                    \"argument_count\": 0\n                }\n            \n            pq = provider_quality[provider]\n            pq[\"argument_count\"] += 1\n            pq[\"avg_quality_score\"] += m[\"quality_score\"]\n            pq[\"avg_word_count\"] += m[\"word_count\"]\n            pq[\"total_citations\"] += m[\"citation_indicators\"]\n            pq[\"total_connectors\"] += m[\"logical_connectors\"]\n        \n        # Calculate averages\n        for provider, metrics in provider_quality.items():\n            count = metrics[\"argument_count\"]\n            metrics[\"avg_quality_score\"] = round(metrics[\"avg_quality_score\"] / count, 2)\n            metrics[\"avg_word_count\"] = round(metrics[\"avg_word_count\"] / count, 2)\n        \n        return {\n            \"individual_arguments\": all_metrics,\n            \"provider_aggregates\": provider_quality\n        }\n\n# Create quality analyzer\nanalyzer = ArgumentQualityAnalyzer()\nprint(\"[W2] Argument Quality Analyzer created\")\nprint(f\"  - Quality scoring: READY\")\nprint(f\"  - Coherence analysis: READY\")\nprint(f\"  - Provider comparison: READY\")\n",
      "dependencies": [
        "W1_METRICS"
      ],
      "status": "completed",
      "duration": 0.0026662349700927734
    },
    {
      "id": "W3_CONSENSUS",
      "name": "Position Tracking & Consensus Detection",
      "code": "\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nclass ConsensusDetector:\n    \"\"\"Tracks positions and detects emerging consensus\"\"\"\n    \n    def __init__(self):\n        self.position_map = defaultdict(list)\n        self.consensus_points = []\n    \n    def track_positions(self, debate_results: Dict):\n        \"\"\"Track all positions across debate\"\"\"\n        for round_data in debate_results.get(\"rounds\", []):\n            for arg in round_data.get(\"arguments\", []):\n                position = arg.get(\"position\")\n                provider = arg.get(\"provider\")\n                argument = arg.get(\"argument\", \"\")\n                \n                self.position_map[position].append({\n                    \"provider\": provider,\n                    \"argument\": argument,\n                    \"round\": arg.get(\"round_number\")\n                })\n    \n    def detect_consensus(self, debate_results: Dict) -> Dict:\n        \"\"\"\n        Detect consensus points across positions\n        \n        Looks for:\n        - Common themes in 'for' arguments\n        - Common themes in 'against' arguments  \n        - Synthesis points\n        - Agreement areas\n        \"\"\"\n        self.track_positions(debate_results)\n        \n        consensus = {\n            \"for_themes\": self._extract_themes(self.position_map.get(\"for\", [])),\n            \"against_themes\": self._extract_themes(self.position_map.get(\"against\", [])),\n            \"synthesis_points\": self._extract_themes(self.position_map.get(\"synthesis\", [])),\n            \"common_ground\": []\n        }\n        \n        # Find common ground (simplified keyword matching)\n        for_keywords = self._extract_keywords(self.position_map.get(\"for\", []))\n        against_keywords = self._extract_keywords(self.position_map.get(\"against\", []))\n        \n        common_keywords = set(for_keywords) & set(against_keywords)\n        consensus[\"common_ground\"] = list(common_keywords)\n        \n        return consensus\n    \n    def _extract_themes(self, arguments: List[Dict]) -> List[str]:\n        \"\"\"Extract key themes from arguments\"\"\"\n        # Simplified theme extraction (word frequency)\n        all_words = []\n        for arg in arguments:\n            words = arg.get(\"argument\", \"\").lower().split()\n            words = [w.strip('.,!?;:') for w in words if len(w) > 5]\n            all_words.extend(words)\n        \n        # Count frequency\n        word_freq = defaultdict(int)\n        for word in all_words:\n            word_freq[word] += 1\n        \n        # Top themes\n        themes = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n        return [theme[0] for theme in themes]\n    \n    def _extract_keywords(self, arguments: List[Dict]) -> List[str]:\n        \"\"\"Extract keywords from arguments\"\"\"\n        keywords = set()\n        for arg in arguments:\n            words = arg.get(\"argument\", \"\").lower().split()\n            words = [w.strip('.,!?;:') for w in words if len(w) > 6]\n            keywords.update(words[:20])  # Top 20 per argument\n        return list(keywords)\n\n# Create consensus detector\ndetector = ConsensusDetector()\nprint(\"[W3] Consensus Detector created\")\nprint(f\"  - Position tracking: READY\")\nprint(f\"  - Theme extraction: READY\")\nprint(f\"  - Common ground detection: READY\")\n",
      "dependencies": [
        "W1_METRICS",
        "W2_QUALITY"
      ],
      "status": "completed",
      "duration": 0.0010726451873779297
    },
    {
      "id": "W4_VISUALIZER",
      "name": "Debate Visualization & Report Generator",
      "code": "\nfrom datetime import datetime\nimport json\n\nclass DebateVisualizer:\n    \"\"\"Generates visualizations and reports for debate results\"\"\"\n    \n    def __init__(self):\n        self.report_data = {}\n    \n    def generate_html_report(self, debate_results: Dict, filename: str = None) -> str:\n        \"\"\"Generate HTML visualization report\"\"\"\n        \n        if filename is None:\n            filename = f\"debate_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n        \n        html = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Debate Analysis Report</title>\n    <style>\n        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}\n        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; }}\n        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n        h2 {{ color: #34495e; margin-top: 30px; }}\n        .metadata {{ background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 20px 0; }}\n        .round {{ border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; }}\n        .argument {{ background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 5px; }}\n        .provider {{ font-weight: bold; color: #2980b9; }}\n        .position {{ display: inline-block; padding: 3px 10px; border-radius: 3px; font-size: 12px; }}\n        .for {{ background: #2ecc71; color: white; }}\n        .against {{ background: #e74c3c; color: white; }}\n        .synthesis {{ background: #9b59b6; color: white; }}\n        .metrics {{ display: flex; justify-content: space-around; margin: 20px 0; }}\n        .metric {{ text-align: center; }}\n        .metric-value {{ font-size: 32px; font-weight: bold; color: #3498db; }}\n        .metric-label {{ color: #7f8c8d; font-size: 14px; }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Adversarial LLM Debate Analysis</h1>\n        \n        <div class=\"metadata\">\n            <p><strong>Debate ID:</strong> {debate_results.get('debate_id', 'N/A')}</p>\n            <p><strong>Topic:</strong> {debate_results.get('topic', 'N/A')}</p>\n            <p><strong>Timestamp:</strong> {debate_results.get('timestamp', 'N/A')}</p>\n            <p><strong>Duration:</strong> {debate_results.get('duration_ms', 0)/1000:.2f}s</p>\n        </div>\n        \n        <h2>Overview Metrics</h2>\n        <div class=\"metrics\">\n            <div class=\"metric\">\n                <div class=\"metric-value\">{debate_results.get('total_rounds', 0)}</div>\n                <div class=\"metric-label\">Rounds</div>\n            </div>\n            <div class=\"metric\">\n                <div class=\"metric-value\">{len(debate_results.get('participants', []))}</div>\n                <div class=\"metric-label\">Providers</div>\n            </div>\n            <div class=\"metric\">\n                <div class=\"metric-value\">{sum(len(r.get('arguments', [])) for r in debate_results.get('rounds', []))}</div>\n                <div class=\"metric-label\">Arguments</div>\n            </div>\n        </div>\n        \n        <h2>Participants</h2>\n        <ul>\n\"\"\"\n        \n        for participant in debate_results.get('participants', []):\n            html += f\"            <li><strong>{participant.get('provider')}:</strong> {participant.get('model')}</li>\\n\"\n        \n        html += \"\"\"        </ul>\n        \n        <h2>Debate Rounds</h2>\n\"\"\"\n        \n        for round_data in debate_results.get('rounds', []):\n            html += f\"\"\"        <div class=\"round\">\n            <h3>Round {round_data.get('round_id')}</h3>\n            <p><em>Duration: {round_data.get('duration_ms', 0)/1000:.2f}s</em></p>\n\"\"\"\n            \n            for arg in round_data.get('arguments', []):\n                position_class = arg.get('position', 'synthesis')\n                html += f\"\"\"            <div class=\"argument\">\n                <div>\n                    <span class=\"provider\">{arg.get('provider')}</span>\n                    <span class=\"position {position_class}\">{arg.get('position').upper()}</span>\n                    <span style=\"color: #95a5a6; font-size: 12px;\">\n                        ({arg.get('latency_ms', 0):.0f}ms, {arg.get('token_count', 0)} tokens)\n                    </span>\n                </div>\n                <p>{arg.get('argument', '')[:500]}{'...' if len(arg.get('argument', '')) > 500 else ''}</p>\n            </div>\n\"\"\"\n            \n            html += \"        </div>\\n\"\n        \n        html += \"\"\"    </div>\n</body>\n</html>\n\"\"\"\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            f.write(html)\n        \n        return filename\n    \n    def generate_markdown_summary(self, debate_results: Dict, filename: str = None) -> str:\n        \"\"\"Generate markdown summary\"\"\"\n        \n        if filename is None:\n            filename = f\"debate_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n        \n        md = f\"\"\"# Adversarial LLM Debate Summary\n\n## Metadata\n- **Debate ID:** {debate_results.get('debate_id', 'N/A')}\n- **Topic:** {debate_results.get('topic', 'N/A')}\n- **Timestamp:** {debate_results.get('timestamp', 'N/A')}\n- **Duration:** {debate_results.get('duration_ms', 0)/1000:.2f}s\n- **Rounds:** {debate_results.get('total_rounds', 0)}\n\n## Participants\n\"\"\"\n        \n        for participant in debate_results.get('participants', []):\n            md += f\"- **{participant.get('provider')}:** {participant.get('model')}\\n\"\n        \n        md += \"\\n## Debate Rounds\\n\\n\"\n        \n        for round_data in debate_results.get('rounds', []):\n            md += f\"### Round {round_data.get('round_id')}\\n\\n\"\n            \n            for arg in round_data.get('arguments', []):\n                md += f\"\"\"#### {arg.get('provider')} ({arg.get('position').upper()})\n*{arg.get('latency_ms', 0):.0f}ms, {arg.get('token_count', 0)} tokens*\n\n{arg.get('argument', '')}\n\n---\n\n\"\"\"\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            f.write(md)\n        \n        return filename\n\n# Create visualizer\nvisualizer = DebateVisualizer()\nprint(\"[W4] Debate Visualizer created\")\nprint(f\"  - HTML report generation: READY\")\nprint(f\"  - Markdown summary: READY\")\nprint(f\"  - Export formats: READY\")\n",
      "dependencies": [
        "W1_METRICS",
        "W2_QUALITY",
        "W3_CONSENSUS"
      ],
      "status": "failed",
      "error": "name 'Dict' is not defined",
      "duration": 0.0009942054748535156
    },
    {
      "id": "W5_OPTIMIZER",
      "name": "Debate Strategy Optimizer",
      "code": "\nfrom typing import Dict, List\n\nclass DebateOptimizer:\n    \"\"\"Optimizes debate strategies based on performance data\"\"\"\n    \n    def __init__(self):\n        self.optimizations = {}\n    \n    def analyze_provider_performance(self, debate_results: Dict) -> Dict:\n        \"\"\"Analyze which providers performed best\"\"\"\n        \n        provider_stats = {}\n        \n        for round_data in debate_results.get('rounds', []):\n            for arg in round_data.get('arguments', []):\n                provider = arg.get('provider')\n                \n                if provider not in provider_stats:\n                    provider_stats[provider] = {\n                        \"total_latency\": 0,\n                        \"total_tokens\": 0,\n                        \"argument_count\": 0,\n                        \"avg_latency\": 0,\n                        \"avg_tokens\": 0,\n                        \"efficiency_score\": 0\n                    }\n                \n                stats = provider_stats[provider]\n                stats[\"total_latency\"] += arg.get('latency_ms', 0)\n                stats[\"total_tokens\"] += arg.get('token_count', 0)\n                stats[\"argument_count\"] += 1\n        \n        # Calculate averages and efficiency\n        for provider, stats in provider_stats.items():\n            count = stats[\"argument_count\"]\n            stats[\"avg_latency\"] = stats[\"total_latency\"] / count\n            stats[\"avg_tokens\"] = stats[\"total_tokens\"] / count\n            \n            # Efficiency = tokens / latency (higher is better)\n            stats[\"efficiency_score\"] = (\n                stats[\"avg_tokens\"] / stats[\"avg_latency\"] * 1000 \n                if stats[\"avg_latency\"] > 0 else 0\n            )\n        \n        return provider_stats\n    \n    def recommend_optimizations(self, provider_stats: Dict) -> List[Dict]:\n        \"\"\"Generate optimization recommendations\"\"\"\n        \n        recommendations = []\n        \n        # Sort by efficiency\n        sorted_providers = sorted(\n            provider_stats.items(),\n            key=lambda x: x[1][\"efficiency_score\"],\n            reverse=True\n        )\n        \n        # Recommend prioritizing efficient providers\n        if sorted_providers:\n            best_provider = sorted_providers[0]\n            recommendations.append({\n                \"type\": \"prioritization\",\n                \"recommendation\": f\"Prioritize {best_provider[0]} (efficiency: {best_provider[1]['efficiency_score']:.2f})\",\n                \"details\": best_provider[1]\n            })\n        \n        # Identify slow providers\n        for provider, stats in provider_stats.items():\n            if stats[\"avg_latency\"] > 5000:  # > 5 seconds\n                recommendations.append({\n                    \"type\": \"performance\",\n                    \"recommendation\": f\"Optimize or reduce usage of {provider} (avg latency: {stats['avg_latency']:.0f}ms)\",\n                    \"details\": stats\n                })\n        \n        # Token usage recommendations\n        avg_tokens = sum(s[\"avg_tokens\"] for s in provider_stats.values()) / len(provider_stats)\n        for provider, stats in provider_stats.items():\n            if stats[\"avg_tokens\"] < avg_tokens * 0.5:\n                recommendations.append({\n                    \"type\": \"quality\",\n                    \"recommendation\": f\"{provider} generating shorter responses ({stats['avg_tokens']:.0f} vs {avg_tokens:.0f} avg)\",\n                    \"details\": stats\n                })\n        \n        return recommendations\n\n# Create optimizer\noptimizer = DebateOptimizer()\nprint(\"[W5] Debate Optimizer created\")\nprint(f\"  - Performance analysis: READY\")\nprint(f\"  - Strategy recommendations: READY\")\nprint(f\"  - Efficiency scoring: READY\")\n",
      "dependencies": [
        "W1_METRICS",
        "W2_QUALITY"
      ],
      "status": "completed",
      "duration": 0.0012013912200927734
    },
    {
      "id": "W6_COORDINATOR",
      "name": "Infrastructure Integration Coordinator",
      "code": "\nimport json\nfrom datetime import datetime\n\nclass InfrastructureCoordinator:\n    \"\"\"Coordinates all infrastructure components\"\"\"\n    \n    def __init__(self):\n        self.components = {\n            \"metrics_collector\": \"W1_METRICS\",\n            \"quality_analyzer\": \"W2_QUALITY\",\n            \"consensus_detector\": \"W3_CONSENSUS\",\n            \"visualizer\": \"W4_VISUALIZER\",\n            \"optimizer\": \"W5_OPTIMIZER\"\n        }\n        \n        self.integration_status = {}\n    \n    def coordinate_analysis_pipeline(self, debate_results: Dict) -> Dict:\n        \"\"\"\n        Coordinate full analysis pipeline\n        \n        Flow:\n        1. Collect metrics (W1)\n        2. Analyze quality (W2)\n        3. Detect consensus (W3)\n        4. Generate visualizations (W4)\n        5. Optimize strategies (W5)\n        \"\"\"\n        \n        pipeline_results = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"debate_id\": debate_results.get(\"debate_id\"),\n            \"pipeline_stages\": []\n        }\n        \n        # Stage 1: Metrics Collection\n        pipeline_results[\"pipeline_stages\"].append({\n            \"stage\": \"metrics_collection\",\n            \"component\": \"W1_METRICS\",\n            \"status\": \"configured\"\n        })\n        \n        # Stage 2: Quality Analysis\n        pipeline_results[\"pipeline_stages\"].append({\n            \"stage\": \"quality_analysis\",\n            \"component\": \"W2_QUALITY\",\n            \"status\": \"configured\"\n        })\n        \n        # Stage 3: Consensus Detection\n        pipeline_results[\"pipeline_stages\"].append({\n            \"stage\": \"consensus_detection\",\n            \"component\": \"W3_CONSENSUS\",\n            \"status\": \"configured\"\n        })\n        \n        # Stage 4: Visualization\n        pipeline_results[\"pipeline_stages\"].append({\n            \"stage\": \"visualization\",\n            \"component\": \"W4_VISUALIZER\",\n            \"status\": \"configured\"\n        })\n        \n        # Stage 5: Optimization\n        pipeline_results[\"pipeline_stages\"].append({\n            \"stage\": \"optimization\",\n            \"component\": \"W5_OPTIMIZER\",\n            \"status\": \"configured\"\n        })\n        \n        return pipeline_results\n    \n    def save_integration_report(self, pipeline_results: Dict):\n        \"\"\"Save integration coordination report\"\"\"\n        \n        filename = f\"infrastructure_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        \n        with open(filename, 'w') as f:\n            json.dump(pipeline_results, f, indent=2)\n        \n        return filename\n\n# Create coordinator\ncoordinator = InfrastructureCoordinator()\nprint(\"[W6] Infrastructure Coordinator created\")\nprint(f\"  - Pipeline coordination: READY\")\nprint(f\"  - Component integration: READY\")\nprint(f\"  - Analysis flow: READY\")\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"INFRASTRUCTURE COMPONENTS SUMMARY\")\nprint(\"=\"*60)\nprint(\"W1: Metrics Collector - Tracks provider performance\")\nprint(\"W2: Quality Analyzer - Evaluates argument quality\")\nprint(\"W3: Consensus Detector - Finds common ground\")\nprint(\"W4: Visualizer - Generates reports (HTML/MD)\")\nprint(\"W5: Optimizer - Recommends strategy improvements\")\nprint(\"W6: Coordinator - Integrates all components\")\nprint(\"=\"*60)\n",
      "dependencies": [
        "W1_METRICS",
        "W2_QUALITY",
        "W3_CONSENSUS",
        "W4_VISUALIZER",
        "W5_OPTIMIZER"
      ],
      "status": "failed",
      "error": "name 'Dict' is not defined",
      "duration": 0.0007319450378417969
    }
  ]
}